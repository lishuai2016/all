Hadoop sink

a)创建agent配置文件

a1.sources = r1
a1.sinks = k1
a1.channels = c1
# Describe/configure the source
a1.sources.r1.type = syslogtcp
a1.sources.r1.port = 5140
a1.sources.r1.host = localhost
a1.sources.r1.channels = c1
# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.path = hdfs://bigdata0220/tmp/flume/syslogtcp
a1.sinks.k1.hdfs.filePrefix = Syslog
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = minute
# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1


b)启动flume agent a1
bin/flume-ng agent -c conf -f conf/hadoop.conf -n a1 -Dflume.root.logger=INFO,console


c)测试产生syslog
echo "hello idoall flume -> hadoop testing one" | nc localhost 5140


d) 执行结果
2017-03-06 13:47:59,028 (New I/O worker #1) [WARN - org.apache.flume.source.SyslogUtils.buildEvent(SyslogUtils.java:316)] Event created from Invalid Syslog data.
2017-03-06 13:47:59,472 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.hdfs.HDFSSequenceFile.configure(HDFSSequenceFile.java:63)] writeFormat = Writable, UseRawLocalFileSystem = false
2017-03-06 13:47:59,752 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:234)] Creating hdfs://bigdata0220/tmp/flume/syslogtcp/Syslog.1488779279469.tmp

查看文件目录
[root@m1 flume-ng]#  hdfs dfs -ls  /tmp/flume/
Found 1 items
drwxr-xr-x   - root root          0 2017-03-06 13:48 /tmp/flume/syslogtcp
[root@m1 flume-ng]#  hdfs dfs -ls -R /tmp/flume/
drwxr-xr-x   - root root          0 2017-03-06 13:48 /tmp/flume/syslogtcp
-rw-r--r--   3 root root        155 2017-03-06 13:48 /tmp/flume/syslogtcp/Syslog.1488779279469

