
<!-- TOC -->

- [第132讲-基于Spark 2.0的用户活跃度分析：模块介绍以及交互式用户行为分析系统的解释](#第132讲-基于spark-20的用户活跃度分析模块介绍以及交互式用户行为分析系统的解释)
    - [1、升级介绍](#1升级介绍)
    - [2、重要的题外话交互式用户行为分析系统](#2重要的题外话交互式用户行为分析系统)
    - [3、模块介绍](#3模块介绍)
- [第133讲-基于Spark 2.0的用户活跃度分析：统计指定时间内访问次数最多的10个用户](#第133讲-基于spark-20的用户活跃度分析统计指定时间内访问次数最多的10个用户)
- [第134讲-基于Spark 2.0的用户活跃度分析：统计指定时间内购买金额最多的10个用户](#第134讲-基于spark-20的用户活跃度分析统计指定时间内购买金额最多的10个用户)
- [第135讲-基于Spark 2.0的用户活跃度分析：统计最近一个周期相比上一个周期访问次数增长最多的10个用户](#第135讲-基于spark-20的用户活跃度分析统计最近一个周期相比上一个周期访问次数增长最多的10个用户)
- [第136讲-基于Spark 2.0的用户活跃度分析：统计最近一个周期相比上一个周期购买金额增长最多的10个用户](#第136讲-基于spark-20的用户活跃度分析统计最近一个周期相比上一个周期购买金额增长最多的10个用户)
- [第137讲-基于Spark 2.0的用户活跃度分析：统计指定注册时间范围内头7天访问次数最高的10个用户](#第137讲-基于spark-20的用户活跃度分析统计指定注册时间范围内头7天访问次数最高的10个用户)
- [第138讲-基于Spark 2.0的用户活跃度分析：统计指定注册时间范围内头7天购买金额最高的10个用户](#第138讲-基于spark-20的用户活跃度分析统计指定注册时间范围内头7天购买金额最高的10个用户)

<!-- /TOC -->

# 第132讲-基于Spark 2.0的用户活跃度分析：模块介绍以及交互式用户行为分析系统的解释


## 1、升级介绍
本次升级，主要是考虑将最新的spark 2.0相关的内容引入升级的项目开发中，让大家体验如何用最新的技术进行项目开发。

之前几讲主要是《Spark从入门到精通》的Spark 2.0分析的一些内容，这里为了让只购买《Spark大型项目实战》的同学能够了解Spark 2.0的一些基本的思想，所以附赠了那些视频。

spark 2.0中，目前就我来看，主要最有价值的就是Dataset相关API的成熟，能够提供比RDD这种原始API更高level的抽象和API，更加方便我们的数据开发工作。此外，就是更加完善了对SQL语法的支持，能够让我们更加便利地使用SQL进行大数据的分析工作。

由于SQL语法都是最基础的东西，因此觉得没有必要花费太多篇幅讲解。本次升级主要关注Spark 2.0的Dataset API，特地挑选了一个用户行为分析中的典型例子，用户活跃度分析模块，来讲解如何使用Dataset进行项目的开发。


## 2、重要的题外话交互式用户行为分析系统

这里有一个重要的题外话，必须给大家澄清和解释一下，本套课程的特点，以及为什么使用Spark来开发。

最基本的数据分析平台，主要是走离线批处理模式。每天凌晨对昨天的数据进行批量处理和分析，统计出各种指标和报表，放入MySQL等关系型数据库中。第二天，公司的人就可以看到昨天以及昨天以前的数据分析结果。

但是有些场景下，是没有办法提前对数据进行预处理的，比如说随机用户抽取、随机页面转化率分析、随机时间范围内随机商品的分析，等等。就如同我们之前讲解的那些模块一样。此时，就需要提供一个java web系统，供用户在需要的时候，选择对应的查询和分析条件，然后由我们的系统立即运行一个大数据处理分析作业，在最短的时间内给用户提供他们想要的数据。这，就是所谓的交互式用户行为分析。

为什么这套交互式用户行为分析系统，要选择采用Spark技术呢？因为Spark在好好调优之后，性能可以达到同等hive/mapreduce作业的3倍~10倍，大幅度提升性能之后，减少了用户的等待时间，能够提供更好的用户体验。因此，可以说，交互式的数据分析，是Spark的一个非常天然的适用场景。

此外，本套课程是对实际的项目做了很多的合并以及改进的，用尽量少的经典模块，涵盖Spark所有的技术点，让大家可以通过这个项目完成对Spark技术点的实战演练。

还有一件很重要的事情，有人问我，为什么要用Spark RDD API来开发这些复杂的业务逻辑，为什么不直接用SQL？当然，用SQL是可以的，但是要区分一下，SQL主要适用于刚才说的大量离线批处理的ETL作业和统计分析逻辑。统计分析报表以及需求灵活，多变，经常会增加，经常逻辑会变，用SQL是很合适的。但是我们这个是一套系统，和java web配合的系统，模块和需求都是固定的。用SQL的缺点在于，Spark底层自动生成执行计划和代码，我们几乎无法进行深度的调优，遇到问题也不好解决。但是对于我们这种固定需求，少量模块，要求速度和稳定性的系统来说，使用Spark RDD API是最好的选择，因为RDD是最原始的API，我们几乎可以控制一切，包括参数调优以及数据倾斜的重构和优化等等，遇到报错，都是最底层的源码，我们可以很容易进行定位和修复问题。这就是为什么本套系统大量采用Spark RDD API开发复杂业务模块的原因。



## 3、模块介绍
用户活跃度分析

1、指定时间段内访问次数最多的10个用户
2、指定时间段内购买商品金额最多的10个用户
3、最近周期内相对之前一个周期访问次数增长最快的10个用户
4、最近周期内相对之前一个周期购买商品金额增长最快的10个用户
5、指定周期内注册的新用户在头7天访问次数最多的10个用户

Dataset API





# 第133讲-基于Spark 2.0的用户活跃度分析：统计指定时间内访问次数最多的10个用户

```java
package cn.ibeifeng.spark

import org.apache.spark.sql.SparkSession

/**
 * 用户活跃度分析
 * 
 * 我们这次项目课程的升级，也跟spark从入门到精通的升级采取同步，采用scala+eclipse的方式来开发
 * 
 * 我个人而言，还是觉得应该用java去开发spark作业，因为hadoop是最重要的大数据引擎，hadoop mapreduce、hbase，全都是java
 * 整个公司的编程语言技术栈越简单越好，降低人员的招聘和培养的成本
 * 
 * 但是由于市面上，现在大部分的公司，做spark都是采取一种，spark用scala开发，所以开发spark作业也用scala
 * 课程为了跟市场保持同步，后面就随便采取scala来开发了
 * 
 */
object UserActiveDegreeAnalyze {
  
  def main(args: Array[String]) {
    // 如果是按照课程之前的模块，或者整套交互式分析系统的架构，应该先从mysql中提取用户指定的参数（java web系统提供界面供用户选择，然后java web系统将参数写入mysql中）
    // 但是这里已经讲了，之前的环境已经没有了，所以本次升级从简
    // 我们就直接定义一个日期范围，来模拟获取了参数
    val startDate = "2016-09-01";
    val endDate = "2016-11-01";
    
    // 开始写代码
    // spark 2.0具体开发的细节和讲解，全部在从入门到精通中，这里不多说了，直接写代码
    // 要不然如果没有看过从入门到精通的话，就自己去上网查spark 2.0的入门资料
    
    val spark = SparkSession
        .builder()
        .appName("UserActiveDegreeAnalyze")
        .master("local") 
        .config("spark.sql.warehouse.dir", "C:\\Users\\Administrator\\Desktop\\spark-warehouse")
        .getOrCreate()
        
    // 导入spark的隐式转换
    import spark.implicits._
    // 导入spark sql的functions
    import org.apache.spark.sql.functions._
    
    // 获取两份数据集
    val userBaseInfo = spark.read.json("C:\\Users\\Administrator\\Desktop\\user_base_info.json")
    val userActionLog = spark.read.json("C:\\Users\\Administrator\\Desktop\\user_action_log.json")
  
    // 第一个功能：统计指定时间范围内的访问次数最多的10个用户
    // 说明：课程，所以数据不会搞的太多，但是一般来说，pm产品经理，都会抽取100个~1000个用户，供他们仔细分析
    
    userActionLog
        // 第一步：过滤数据，找到指定时间范围内的数据
        .filter("actionTime >= '" + startDate + "' and actionTime <= '" + endDate + "' and actionType = 0") 
        // 第二步：关联对应的用户基本信息数据
        .join(userBaseInfo, userActionLog("userId") === userBaseInfo("userId")) 
        // 第三部：进行分组，按照userid和username
        .groupBy(userBaseInfo("userId"), userBaseInfo("username")) 
        // 第四步：进行聚合
        .agg(count(userActionLog("logId")).alias("actionCount")) 
        // 第五步：进行排序
        .sort($"actionCount".desc)
        // 第六步：抽取指定的条数
        .limit(10) 
        // 第七步：展示结果，因为监简化了，所以说不会再写入mysql
        .show() 
  }
  
}

```



# 第134讲-基于Spark 2.0的用户活跃度分析：统计指定时间内购买金额最多的10个用户

```java
package cn.ibeifeng.spark

import org.apache.spark.sql.SparkSession

/**
 * 用户活跃度分析
 * 
 * 我们这次项目课程的升级，也跟spark从入门到精通的升级采取同步，采用scala+eclipse的方式来开发
 * 
 * 我个人而言，还是觉得应该用java去开发spark作业，因为hadoop是最重要的大数据引擎，hadoop mapreduce、hbase，全都是java
 * 整个公司的编程语言技术栈越简单越好，降低人员的招聘和培养的成本
 * 
 * 但是由于市面上，现在大部分的公司，做spark都是采取一种，spark用scala开发，所以开发spark作业也用scala
 * 课程为了跟市场保持同步，后面就随便采取scala来开发了
 * 
 */
object UserActiveDegreeAnalyze {
  
  def main(args: Array[String]) {
    // 如果是按照课程之前的模块，或者整套交互式分析系统的架构，应该先从mysql中提取用户指定的参数（java web系统提供界面供用户选择，然后java web系统将参数写入mysql中）
    // 但是这里已经讲了，之前的环境已经没有了，所以本次升级从简
    // 我们就直接定义一个日期范围，来模拟获取了参数
    val startDate = "2016-09-01";
    val endDate = "2016-11-01";
    
    // 开始写代码
    // spark 2.0具体开发的细节和讲解，全部在从入门到精通中，这里不多说了，直接写代码
    // 要不然如果没有看过从入门到精通的话，就自己去上网查spark 2.0的入门资料
    
    val spark = SparkSession
        .builder()
        .appName("UserActiveDegreeAnalyze")
        .master("local") 
        .config("spark.sql.warehouse.dir", "C:\\Users\\Administrator\\Desktop\\spark-warehouse")
        .getOrCreate()
        
    // 导入spark的隐式转换
    import spark.implicits._
    // 导入spark sql的functions
    import org.apache.spark.sql.functions._
    
    // 获取两份数据集
    val userBaseInfo = spark.read.json("C:\\Users\\Administrator\\Desktop\\user_base_info.json")
    val userActionLog = spark.read.json("C:\\Users\\Administrator\\Desktop\\user_action_log.json")
  
    // 第一个功能：统计指定时间范围内的访问次数最多的10个用户
    // 说明：课程，所以数据不会搞的太多，但是一般来说，pm产品经理，都会抽取100个~1000个用户，供他们仔细分析
    
//    userActionLog
//        // 第一步：过滤数据，找到指定时间范围内的数据
//        .filter("actionTime >= '" + startDate + "' and actionTime <= '" + endDate + "' and actionType = 0") 
//        // 第二步：关联对应的用户基本信息数据
//        .join(userBaseInfo, userActionLog("userId") === userBaseInfo("userId")) 
//        // 第三部：进行分组，按照userid和username
//        .groupBy(userBaseInfo("userId"), userBaseInfo("username")) 
//        // 第四步：进行聚合
//        .agg(count(userActionLog("logId")).alias("actionCount")) 
//        // 第五步：进行排序
//        .sort($"actionCount".desc)
//        // 第六步：抽取指定的条数
//        .limit(10) 
//        // 第七步：展示结果，因为监简化了，所以说不会再写入mysql
//        .show()
    
    // 第二个功能：获取指定时间范围内购买金额最多的10个用户
    // 对金额进行处理的函数讲解
    // feature，技术点的讲解：嵌套函数的使用
    userActionLog
        .filter("actionTime >= '" + startDate + "' and actionTime <= '" + endDate + "' and actionType = 1") 
        .join(userBaseInfo, userActionLog("userId") === userBaseInfo("userId")) 
        .groupBy(userBaseInfo("userId"), userBaseInfo("username"))
        .agg(round(sum(userActionLog("purchaseMoney")),2).alias("totalPurchaseMoney")) 
        .sort($"totalPurchaseMoney".desc)
        .limit(10)
        .show() 
  }
  
}

```



# 第135讲-基于Spark 2.0的用户活跃度分析：统计最近一个周期相比上一个周期访问次数增长最多的10个用户

```java
package cn.ibeifeng.spark

import org.apache.spark.sql.SparkSession

/**
 * 用户活跃度分析
 * 
 * 我们这次项目课程的升级，也跟spark从入门到精通的升级采取同步，采用scala+eclipse的方式来开发
 * 
 * 我个人而言，还是觉得应该用java去开发spark作业，因为hadoop是最重要的大数据引擎，hadoop mapreduce、hbase，全都是java
 * 整个公司的编程语言技术栈越简单越好，降低人员的招聘和培养的成本
 * 
 * 但是由于市面上，现在大部分的公司，做spark都是采取一种，spark用scala开发，所以开发spark作业也用scala
 * 课程为了跟市场保持同步，后面就随便采取scala来开发了
 * 
 */
object UserActiveDegreeAnalyze {
  
  case class UserActionLog(logId: Long, userId: Long, actionTime: String, actionType: Long, purchaseMoney: Double)
  case class UserActionLogVO(logId: Long, userId: Long, actionValue: Long)
  
  def main(args: Array[String]) {
    // 如果是按照课程之前的模块，或者整套交互式分析系统的架构，应该先从mysql中提取用户指定的参数（java web系统提供界面供用户选择，然后java web系统将参数写入mysql中）
    // 但是这里已经讲了，之前的环境已经没有了，所以本次升级从简
    // 我们就直接定义一个日期范围，来模拟获取了参数
    val startDate = "2016-09-01";
    val endDate = "2016-11-01";
    
    // 开始写代码
    // spark 2.0具体开发的细节和讲解，全部在从入门到精通中，这里不多说了，直接写代码
    // 要不然如果没有看过从入门到精通的话，就自己去上网查spark 2.0的入门资料
    
    val spark = SparkSession
        .builder()
        .appName("UserActiveDegreeAnalyze")
        .master("local") 
        .config("spark.sql.warehouse.dir", "C:\\Users\\Administrator\\Desktop\\spark-warehouse")
        .getOrCreate()
        
    // 导入spark的隐式转换
    import spark.implicits._
    // 导入spark sql的functions
    import org.apache.spark.sql.functions._
    
    // 获取两份数据集
    val userBaseInfo = spark.read.json("C:\\Users\\Administrator\\Desktop\\user_base_info.json")
    val userActionLog = spark.read.json("C:\\Users\\Administrator\\Desktop\\user_action_log.json")
  
    // 第一个功能：统计指定时间范围内的访问次数最多的10个用户
    // 说明：课程，所以数据不会搞的太多，但是一般来说，pm产品经理，都会抽取100个~1000个用户，供他们仔细分析
    
//    userActionLog
//        // 第一步：过滤数据，找到指定时间范围内的数据
//        .filter("actionTime >= '" + startDate + "' and actionTime <= '" + endDate + "' and actionType = 0") 
//        // 第二步：关联对应的用户基本信息数据
//        .join(userBaseInfo, userActionLog("userId") === userBaseInfo("userId")) 
//        // 第三部：进行分组，按照userid和username
//        .groupBy(userBaseInfo("userId"), userBaseInfo("username")) 
//        // 第四步：进行聚合
//        .agg(count(userActionLog("logId")).alias("actionCount")) 
//        // 第五步：进行排序
//        .sort($"actionCount".desc)
//        // 第六步：抽取指定的条数
//        .limit(10) 
//        // 第七步：展示结果，因为监简化了，所以说不会再写入mysql
//        .show()
    
//    // 第二个功能：获取指定时间范围内购买金额最多的10个用户
//    // 对金额进行处理的函数讲解
//    // feature，技术点的讲解：嵌套函数的使用
//    userActionLog
//        .filter("actionTime >= '" + startDate + "' and actionTime <= '" + endDate + "' and actionType = 1") 
//        .join(userBaseInfo, userActionLog("userId") === userBaseInfo("userId")) 
//        .groupBy(userBaseInfo("userId"), userBaseInfo("username"))
//        .agg(round(sum(userActionLog("purchaseMoney")),2).alias("totalPurchaseMoney")) 
//        .sort($"totalPurchaseMoney".desc)
//        .limit(10)
//        .show() 
    
//    // 第三个功能：统计最近一个周期相对上一个周期访问次数增长最多的10个用户
//    
//    // 比如说我们设定一个周期是1个月
//    // 我们有1个用户，叫张三，那么张三在9月份这个周期内总共访问了100次，张三在10月份这个周期内总共访问了200次
//    // 张三这个用户在最近一个周期相比上一个周期，访问次数增长了100次
//    // 每个用户都可以计算出这么一个值
//    // 获取在最近两个周期内，访问次数增长最多的10个用户
//    
//    // 周期，是可以由用户在web界面上填写的，java web系统会写入mysql，我们可以去获取本次执行的周期
//    // 假定1个月，2016-10-01~2016-10-31，上一个周期就是2016-09-01~2016-09-30
//    
//    val userActionLogInFirstPeriod = userActionLog.as[UserActionLog]
//        .filter("actionTime >= '2016-10-01' and actionTime <= '2016-10-31' and actionType = 0") 
//        .map{ userActionLogEntry => UserActionLogVO(userActionLogEntry.logId, userActionLogEntry.userId, 1) }
//    
//    val userActionLogInSecondPeriod = userActionLog.as[UserActionLog]
//        .filter("actionTime >= '2016-01-01' and actionTime <= '2016-09-30' and actionType = 0") 
//        .map{ userActionLogEntry => UserActionLogVO(userActionLogEntry.logId, userActionLogEntry.userId, -1) }
//    
//    val userActionLogDS = userActionLogInFirstPeriod.union(userActionLogInSecondPeriod)
//    
//    userActionLogDS
//        .join(userBaseInfo, userActionLogDS("userId") === userBaseInfo("userId")) 
//        .groupBy(userBaseInfo("userId"), userBaseInfo("username"))
//        .agg(sum(userActionLogDS("actionValue")).alias("actionIncr")) 
//        .sort($"actionIncr".desc)
//        .limit(10)
//        .show()
  }
  
}


```


# 第136讲-基于Spark 2.0的用户活跃度分析：统计最近一个周期相比上一个周期购买金额增长最多的10个用户

```java
package cn.ibeifeng.spark

import org.apache.spark.sql.SparkSession

/**
 * 用户活跃度分析
 * 
 * 我们这次项目课程的升级，也跟spark从入门到精通的升级采取同步，采用scala+eclipse的方式来开发
 * 
 * 我个人而言，还是觉得应该用java去开发spark作业，因为hadoop是最重要的大数据引擎，hadoop mapreduce、hbase，全都是java
 * 整个公司的编程语言技术栈越简单越好，降低人员的招聘和培养的成本
 * 
 * 但是由于市面上，现在大部分的公司，做spark都是采取一种，spark用scala开发，所以开发spark作业也用scala
 * 课程为了跟市场保持同步，后面就随便采取scala来开发了
 * 
 */
object UserActiveDegreeAnalyze {
  
  case class UserActionLog(logId: Long, userId: Long, actionTime: String, actionType: Long, purchaseMoney: Double)
  case class UserActionLogVO(logId: Long, userId: Long, actionValue: Long)
  case class UserActionLogWithPurchaseMoneyVO(logId: Long, userId: Long, purchaseMoney: Double)
  
  def main(args: Array[String]) {
    // 如果是按照课程之前的模块，或者整套交互式分析系统的架构，应该先从mysql中提取用户指定的参数（java web系统提供界面供用户选择，然后java web系统将参数写入mysql中）
    // 但是这里已经讲了，之前的环境已经没有了，所以本次升级从简
    // 我们就直接定义一个日期范围，来模拟获取了参数
    val startDate = "2016-09-01";
    val endDate = "2016-11-01";
    
    // 开始写代码
    // spark 2.0具体开发的细节和讲解，全部在从入门到精通中，这里不多说了，直接写代码
    // 要不然如果没有看过从入门到精通的话，就自己去上网查spark 2.0的入门资料
    
    val spark = SparkSession
        .builder()
        .appName("UserActiveDegreeAnalyze")
        .master("local") 
        .config("spark.sql.warehouse.dir", "C:\\Users\\Administrator\\Desktop\\spark-warehouse")
        .getOrCreate()
        
    // 导入spark的隐式转换
    import spark.implicits._
    // 导入spark sql的functions
    import org.apache.spark.sql.functions._
    
    // 获取两份数据集
    val userBaseInfo = spark.read.json("C:\\Users\\Administrator\\Desktop\\user_base_info.json")
    val userActionLog = spark.read.json("C:\\Users\\Administrator\\Desktop\\user_action_log.json")
  
    // 第一个功能：统计指定时间范围内的访问次数最多的10个用户
    // 说明：课程，所以数据不会搞的太多，但是一般来说，pm产品经理，都会抽取100个~1000个用户，供他们仔细分析
    
//    userActionLog
//        // 第一步：过滤数据，找到指定时间范围内的数据
//        .filter("actionTime >= '" + startDate + "' and actionTime <= '" + endDate + "' and actionType = 0") 
//        // 第二步：关联对应的用户基本信息数据
//        .join(userBaseInfo, userActionLog("userId") === userBaseInfo("userId")) 
//        // 第三部：进行分组，按照userid和username
//        .groupBy(userBaseInfo("userId"), userBaseInfo("username")) 
//        // 第四步：进行聚合
//        .agg(count(userActionLog("logId")).alias("actionCount")) 
//        // 第五步：进行排序
//        .sort($"actionCount".desc)
//        // 第六步：抽取指定的条数
//        .limit(10) 
//        // 第七步：展示结果，因为监简化了，所以说不会再写入mysql
//        .show()
    
//    // 第二个功能：获取指定时间范围内购买金额最多的10个用户
//    // 对金额进行处理的函数讲解
//    // feature，技术点的讲解：嵌套函数的使用
//    userActionLog
//        .filter("actionTime >= '" + startDate + "' and actionTime <= '" + endDate + "' and actionType = 1") 
//        .join(userBaseInfo, userActionLog("userId") === userBaseInfo("userId")) 
//        .groupBy(userBaseInfo("userId"), userBaseInfo("username"))
//        .agg(round(sum(userActionLog("purchaseMoney")),2).alias("totalPurchaseMoney")) 
//        .sort($"totalPurchaseMoney".desc)
//        .limit(10)
//        .show() 
    
//    // 第三个功能：统计最近一个周期相对上一个周期访问次数增长最多的10个用户
//    
//    // 比如说我们设定一个周期是1个月
//    // 我们有1个用户，叫张三，那么张三在9月份这个周期内总共访问了100次，张三在10月份这个周期内总共访问了200次
//    // 张三这个用户在最近一个周期相比上一个周期，访问次数增长了100次
//    // 每个用户都可以计算出这么一个值
//    // 获取在最近两个周期内，访问次数增长最多的10个用户
//    
//    // 周期，是可以由用户在web界面上填写的，java web系统会写入mysql，我们可以去获取本次执行的周期
//    // 假定1个月，2016-10-01~2016-10-31，上一个周期就是2016-09-01~2016-09-30
//    
//    val userActionLogInFirstPeriod = userActionLog.as[UserActionLog]
//        .filter("actionTime >= '2016-10-01' and actionTime <= '2016-10-31' and actionType = 0") 
//        .map{ userActionLogEntry => UserActionLogVO(userActionLogEntry.logId, userActionLogEntry.userId, 1) }
//    
//    val userActionLogInSecondPeriod = userActionLog.as[UserActionLog]
//        .filter("actionTime >= '2016-01-01' and actionTime <= '2016-09-30' and actionType = 0") 
//        .map{ userActionLogEntry => UserActionLogVO(userActionLogEntry.logId, userActionLogEntry.userId, -1) }
//    
//    val userActionLogDS = userActionLogInFirstPeriod.union(userActionLogInSecondPeriod)
//    
//    userActionLogDS
//        .join(userBaseInfo, userActionLogDS("userId") === userBaseInfo("userId")) 
//        .groupBy(userBaseInfo("userId"), userBaseInfo("username"))
//        .agg(sum(userActionLogDS("actionValue")).alias("actionIncr")) 
//        .sort($"actionIncr".desc)
//        .limit(10)
//        .show()
    
//    // 真实的项目中，大量的情况就是这样的，很多作业和代码都是类似的，就是有些地方不太一样而已
//    // 向大家展示真实的项目逻辑，业务
//    // 让大家加强印象，多练习几遍，没什么坏处
//    
//    val userActionLogWithPurchaseMoneyInFirstPeriod = userActionLog.as[UserActionLog]
//        .filter("actionTime >= '2016-10-01' and actionTime <= '2016-10-31' and actionType = 1")
//        .map{ userActionLogEntry => UserActionLogWithPurchaseMoneyVO(userActionLogEntry.logId, userActionLogEntry.userId, userActionLogEntry.purchaseMoney) }
//  
//    val userActionLogWithPurchaseMoneyInSecondPeriod = userActionLog.as[UserActionLog]
//        .filter("actionTime >= '2016-09-01' and actionTime <= '2016-09-30' and actionType = 1")
//        .map{ userActionLogEntry => UserActionLogWithPurchaseMoneyVO(userActionLogEntry.logId, userActionLogEntry.userId, -userActionLogEntry.purchaseMoney) }
//  
//    val userActionLogWithPurchaseMoneyDS = userActionLogWithPurchaseMoneyInFirstPeriod.union(userActionLogWithPurchaseMoneyInSecondPeriod)
//    
//    userActionLogWithPurchaseMoneyDS
//        .join(userBaseInfo, userActionLogWithPurchaseMoneyDS("userId") === userBaseInfo("userId")) 
//        .groupBy(userBaseInfo("userId"), userBaseInfo("username"))
//        .agg(round(sum(userActionLogWithPurchaseMoneyDS("purchaseMoney")), 2).alias("purchaseMoneyIncr")) 
//        .sort($"purchaseMoneyIncr".desc)
//        .limit(10)
//        .show() 
  }
  
}

```


# 第137讲-基于Spark 2.0的用户活跃度分析：统计指定注册时间范围内头7天访问次数最高的10个用户

```java
package cn.ibeifeng.spark

import org.apache.spark.sql.SparkSession

/**
 * 用户活跃度分析
 * 
 * 我们这次项目课程的升级，也跟spark从入门到精通的升级采取同步，采用scala+eclipse的方式来开发
 * 
 * 我个人而言，还是觉得应该用java去开发spark作业，因为hadoop是最重要的大数据引擎，hadoop mapreduce、hbase，全都是java
 * 整个公司的编程语言技术栈越简单越好，降低人员的招聘和培养的成本
 * 
 * 但是由于市面上，现在大部分的公司，做spark都是采取一种，spark用scala开发，所以开发spark作业也用scala
 * 课程为了跟市场保持同步，后面就随便采取scala来开发了
 * 
 */
object UserActiveDegreeAnalyze {
  
  case class UserActionLog(logId: Long, userId: Long, actionTime: String, actionType: Long, purchaseMoney: Double)
  case class UserActionLogVO(logId: Long, userId: Long, actionValue: Long)
  case class UserActionLogWithPurchaseMoneyVO(logId: Long, userId: Long, purchaseMoney: Double)
  
  def main(args: Array[String]) {
    // 如果是按照课程之前的模块，或者整套交互式分析系统的架构，应该先从mysql中提取用户指定的参数（java web系统提供界面供用户选择，然后java web系统将参数写入mysql中）
    // 但是这里已经讲了，之前的环境已经没有了，所以本次升级从简
    // 我们就直接定义一个日期范围，来模拟获取了参数
    val startDate = "2016-09-01";
    val endDate = "2016-11-01";
    
    // 开始写代码
    // spark 2.0具体开发的细节和讲解，全部在从入门到精通中，这里不多说了，直接写代码
    // 要不然如果没有看过从入门到精通的话，就自己去上网查spark 2.0的入门资料
    
    val spark = SparkSession
        .builder()
        .appName("UserActiveDegreeAnalyze")
        .master("local") 
        .config("spark.sql.warehouse.dir", "C:\\Users\\Administrator\\Desktop\\spark-warehouse")
        .getOrCreate()
        
    // 导入spark的隐式转换
    import spark.implicits._
    // 导入spark sql的functions
    import org.apache.spark.sql.functions._
    
    // 获取两份数据集
    val userBaseInfo = spark.read.json("C:\\Users\\Administrator\\Desktop\\user_base_info.json")
    val userActionLog = spark.read.json("C:\\Users\\Administrator\\Desktop\\user_action_log.json")
  
    // 第一个功能：统计指定时间范围内的访问次数最多的10个用户
    // 说明：课程，所以数据不会搞的太多，但是一般来说，pm产品经理，都会抽取100个~1000个用户，供他们仔细分析
    
//    userActionLog
//        // 第一步：过滤数据，找到指定时间范围内的数据
//        .filter("actionTime >= '" + startDate + "' and actionTime <= '" + endDate + "' and actionType = 0") 
//        // 第二步：关联对应的用户基本信息数据
//        .join(userBaseInfo, userActionLog("userId") === userBaseInfo("userId")) 
//        // 第三部：进行分组，按照userid和username
//        .groupBy(userBaseInfo("userId"), userBaseInfo("username")) 
//        // 第四步：进行聚合
//        .agg(count(userActionLog("logId")).alias("actionCount")) 
//        // 第五步：进行排序
//        .sort($"actionCount".desc)
//        // 第六步：抽取指定的条数
//        .limit(10) 
//        // 第七步：展示结果，因为监简化了，所以说不会再写入mysql
//        .show()
    
//    // 第二个功能：获取指定时间范围内购买金额最多的10个用户
//    // 对金额进行处理的函数讲解
//    // feature，技术点的讲解：嵌套函数的使用
//    userActionLog
//        .filter("actionTime >= '" + startDate + "' and actionTime <= '" + endDate + "' and actionType = 1") 
//        .join(userBaseInfo, userActionLog("userId") === userBaseInfo("userId")) 
//        .groupBy(userBaseInfo("userId"), userBaseInfo("username"))
//        .agg(round(sum(userActionLog("purchaseMoney")),2).alias("totalPurchaseMoney")) 
//        .sort($"totalPurchaseMoney".desc)
//        .limit(10)
//        .show() 
    
//    // 第三个功能：统计最近一个周期相对上一个周期访问次数增长最多的10个用户
//    
//    // 比如说我们设定一个周期是1个月
//    // 我们有1个用户，叫张三，那么张三在9月份这个周期内总共访问了100次，张三在10月份这个周期内总共访问了200次
//    // 张三这个用户在最近一个周期相比上一个周期，访问次数增长了100次
//    // 每个用户都可以计算出这么一个值
//    // 获取在最近两个周期内，访问次数增长最多的10个用户
//    
//    // 周期，是可以由用户在web界面上填写的，java web系统会写入mysql，我们可以去获取本次执行的周期
//    // 假定1个月，2016-10-01~2016-10-31，上一个周期就是2016-09-01~2016-09-30
//    
//    val userActionLogInFirstPeriod = userActionLog.as[UserActionLog]
//        .filter("actionTime >= '2016-10-01' and actionTime <= '2016-10-31' and actionType = 0") 
//        .map{ userActionLogEntry => UserActionLogVO(userActionLogEntry.logId, userActionLogEntry.userId, 1) }
//    
//    val userActionLogInSecondPeriod = userActionLog.as[UserActionLog]
//        .filter("actionTime >= '2016-01-01' and actionTime <= '2016-09-30' and actionType = 0") 
//        .map{ userActionLogEntry => UserActionLogVO(userActionLogEntry.logId, userActionLogEntry.userId, -1) }
//    
//    val userActionLogDS = userActionLogInFirstPeriod.union(userActionLogInSecondPeriod)
//    
//    userActionLogDS
//        .join(userBaseInfo, userActionLogDS("userId") === userBaseInfo("userId")) 
//        .groupBy(userBaseInfo("userId"), userBaseInfo("username"))
//        .agg(sum(userActionLogDS("actionValue")).alias("actionIncr")) 
//        .sort($"actionIncr".desc)
//        .limit(10)
//        .show()
    
//    // 真实的项目中，大量的情况就是这样的，很多作业和代码都是类似的，就是有些地方不太一样而已
//    // 向大家展示真实的项目逻辑，业务
//    // 让大家加强印象，多练习几遍，没什么坏处
//    
//    val userActionLogWithPurchaseMoneyInFirstPeriod = userActionLog.as[UserActionLog]
//        .filter("actionTime >= '2016-10-01' and actionTime <= '2016-10-31' and actionType = 1")
//        .map{ userActionLogEntry => UserActionLogWithPurchaseMoneyVO(userActionLogEntry.logId, userActionLogEntry.userId, userActionLogEntry.purchaseMoney) }
//  
//    val userActionLogWithPurchaseMoneyInSecondPeriod = userActionLog.as[UserActionLog]
//        .filter("actionTime >= '2016-09-01' and actionTime <= '2016-09-30' and actionType = 1")
//        .map{ userActionLogEntry => UserActionLogWithPurchaseMoneyVO(userActionLogEntry.logId, userActionLogEntry.userId, -userActionLogEntry.purchaseMoney) }
//  
//    val userActionLogWithPurchaseMoneyDS = userActionLogWithPurchaseMoneyInFirstPeriod.union(userActionLogWithPurchaseMoneyInSecondPeriod)
//    
//    userActionLogWithPurchaseMoneyDS
//        .join(userBaseInfo, userActionLogWithPurchaseMoneyDS("userId") === userBaseInfo("userId")) 
//        .groupBy(userBaseInfo("userId"), userBaseInfo("username"))
//        .agg(round(sum(userActionLogWithPurchaseMoneyDS("purchaseMoney")), 2).alias("purchaseMoneyIncr")) 
//        .sort($"purchaseMoneyIncr".desc)
//        .limit(10)
//        .show() 
    
    // 统计指定注册时间范围内头7天访问次数最高的10个用户
    // 举例，用户通过web界面指定的注册范围是2016-10-01~2016-10-31
    
    userActionLog
        .join(userBaseInfo, userActionLog("userId") === userBaseInfo("userId")) 
        .filter(userBaseInfo("registTime") >= "2016-10-01"
            && userBaseInfo("registTime") <= "2016-10-31"
            && userActionLog("actionTime") >= userBaseInfo("registTime")
            && userActionLog("actionTime") <= date_add(userBaseInfo("registTime"), 7))
        .groupBy(userBaseInfo("userId"), userBaseInfo("username"))
        .agg(count(userActionLog("logId")).alias("actionCount"))
        .sort($"actionCount".desc)
        .limit(10)
        .show()
  }
  
}


```
# 第138讲-基于Spark 2.0的用户活跃度分析：统计指定注册时间范围内头7天购买金额最高的10个用户

```java
package cn.ibeifeng.spark

import org.apache.spark.sql.SparkSession

/**
 * 用户活跃度分析
 * 
 * 我们这次项目课程的升级，也跟spark从入门到精通的升级采取同步，采用scala+eclipse的方式来开发
 * 
 * 我个人而言，还是觉得应该用java去开发spark作业，因为hadoop是最重要的大数据引擎，hadoop mapreduce、hbase，全都是java
 * 整个公司的编程语言技术栈越简单越好，降低人员的招聘和培养的成本
 * 
 * 但是由于市面上，现在大部分的公司，做spark都是采取一种，spark用scala开发，所以开发spark作业也用scala
 * 课程为了跟市场保持同步，后面就随便采取scala来开发了
 * 
 */
object UserActiveDegreeAnalyze {
  
  case class UserActionLog(logId: Long, userId: Long, actionTime: String, actionType: Long, purchaseMoney: Double)
  case class UserActionLogVO(logId: Long, userId: Long, actionValue: Long)
  case class UserActionLogWithPurchaseMoneyVO(logId: Long, userId: Long, purchaseMoney: Double)
  
  def main(args: Array[String]) {
    // 如果是按照课程之前的模块，或者整套交互式分析系统的架构，应该先从mysql中提取用户指定的参数（java web系统提供界面供用户选择，然后java web系统将参数写入mysql中）
    // 但是这里已经讲了，之前的环境已经没有了，所以本次升级从简
    // 我们就直接定义一个日期范围，来模拟获取了参数
    val startDate = "2016-09-01";
    val endDate = "2016-11-01";
    
    // 开始写代码
    // spark 2.0具体开发的细节和讲解，全部在从入门到精通中，这里不多说了，直接写代码
    // 要不然如果没有看过从入门到精通的话，就自己去上网查spark 2.0的入门资料
    
    val spark = SparkSession
        .builder()
        .appName("UserActiveDegreeAnalyze")
        .master("local") 
        .config("spark.sql.warehouse.dir", "C:\\Users\\Administrator\\Desktop\\spark-warehouse")
        .getOrCreate()
        
    // 导入spark的隐式转换
    import spark.implicits._
    // 导入spark sql的functions
    import org.apache.spark.sql.functions._
    
    // 获取两份数据集
    val userBaseInfo = spark.read.json("C:\\Users\\Administrator\\Desktop\\user_base_info.json")
    val userActionLog = spark.read.json("C:\\Users\\Administrator\\Desktop\\user_action_log.json")
  
    // 第一个功能：统计指定时间范围内的访问次数最多的10个用户
    // 说明：课程，所以数据不会搞的太多，但是一般来说，pm产品经理，都会抽取100个~1000个用户，供他们仔细分析
    
    userActionLog
        // 第一步：过滤数据，找到指定时间范围内的数据
        .filter("actionTime >= '" + startDate + "' and actionTime <= '" + endDate + "' and actionType = 0") 
        // 第二步：关联对应的用户基本信息数据
        .join(userBaseInfo, userActionLog("userId") === userBaseInfo("userId")) 
        // 第三部：进行分组，按照userid和username
        .groupBy(userBaseInfo("userId"), userBaseInfo("username")) 
        // 第四步：进行聚合
        .agg(count(userActionLog("logId")).alias("actionCount")) 
        // 第五步：进行排序
        .sort($"actionCount".desc)
        // 第六步：抽取指定的条数
        .limit(10) 
        // 第七步：展示结果，因为监简化了，所以说不会再写入mysql
        .show()
    
    // 第二个功能：获取指定时间范围内购买金额最多的10个用户
    // 对金额进行处理的函数讲解
    // feature，技术点的讲解：嵌套函数的使用
    userActionLog
        .filter("actionTime >= '" + startDate + "' and actionTime <= '" + endDate + "' and actionType = 1") 
        .join(userBaseInfo, userActionLog("userId") === userBaseInfo("userId")) 
        .groupBy(userBaseInfo("userId"), userBaseInfo("username"))
        .agg(round(sum(userActionLog("purchaseMoney")),2).alias("totalPurchaseMoney")) 
        .sort($"totalPurchaseMoney".desc)
        .limit(10)
        .show() 
    
    // 第三个功能：统计最近一个周期相对上一个周期访问次数增长最多的10个用户
    
    // 比如说我们设定一个周期是1个月
    // 我们有1个用户，叫张三，那么张三在9月份这个周期内总共访问了100次，张三在10月份这个周期内总共访问了200次
    // 张三这个用户在最近一个周期相比上一个周期，访问次数增长了100次
    // 每个用户都可以计算出这么一个值
    // 获取在最近两个周期内，访问次数增长最多的10个用户
    
    // 周期，是可以由用户在web界面上填写的，java web系统会写入mysql，我们可以去获取本次执行的周期
    // 假定1个月，2016-10-01~2016-10-31，上一个周期就是2016-09-01~2016-09-30
    
    val userActionLogInFirstPeriod = userActionLog.as[UserActionLog]
        .filter("actionTime >= '2016-10-01' and actionTime <= '2016-10-31' and actionType = 0") 
        .map{ userActionLogEntry => UserActionLogVO(userActionLogEntry.logId, userActionLogEntry.userId, 1) }
    
    val userActionLogInSecondPeriod = userActionLog.as[UserActionLog]
        .filter("actionTime >= '2016-01-01' and actionTime <= '2016-09-30' and actionType = 0") 
        .map{ userActionLogEntry => UserActionLogVO(userActionLogEntry.logId, userActionLogEntry.userId, -1) }
    
    val userActionLogDS = userActionLogInFirstPeriod.union(userActionLogInSecondPeriod)
    
    userActionLogDS
        .join(userBaseInfo, userActionLogDS("userId") === userBaseInfo("userId")) 
        .groupBy(userBaseInfo("userId"), userBaseInfo("username"))
        .agg(sum(userActionLogDS("actionValue")).alias("actionIncr")) 
        .sort($"actionIncr".desc)
        .limit(10)
        .show()
    
    // 真实的项目中，大量的情况就是这样的，很多作业和代码都是类似的，就是有些地方不太一样而已
    // 向大家展示真实的项目逻辑，业务
    // 让大家加强印象，多练习几遍，没什么坏处
    
    val userActionLogWithPurchaseMoneyInFirstPeriod = userActionLog.as[UserActionLog]
        .filter("actionTime >= '2016-10-01' and actionTime <= '2016-10-31' and actionType = 1")
        .map{ userActionLogEntry => UserActionLogWithPurchaseMoneyVO(userActionLogEntry.logId, userActionLogEntry.userId, userActionLogEntry.purchaseMoney) }
  
    val userActionLogWithPurchaseMoneyInSecondPeriod = userActionLog.as[UserActionLog]
        .filter("actionTime >= '2016-09-01' and actionTime <= '2016-09-30' and actionType = 1")
        .map{ userActionLogEntry => UserActionLogWithPurchaseMoneyVO(userActionLogEntry.logId, userActionLogEntry.userId, -userActionLogEntry.purchaseMoney) }
  
    val userActionLogWithPurchaseMoneyDS = userActionLogWithPurchaseMoneyInFirstPeriod.union(userActionLogWithPurchaseMoneyInSecondPeriod)
    
    userActionLogWithPurchaseMoneyDS
        .join(userBaseInfo, userActionLogWithPurchaseMoneyDS("userId") === userBaseInfo("userId")) 
        .groupBy(userBaseInfo("userId"), userBaseInfo("username"))
        .agg(round(sum(userActionLogWithPurchaseMoneyDS("purchaseMoney")), 2).alias("purchaseMoneyIncr")) 
        .sort($"purchaseMoneyIncr".desc)
        .limit(10)
        .show() 
    
    // 统计指定注册时间范围内头7天访问次数最高的10个用户
    // 举例，用户通过web界面指定的注册范围是2016-10-01~2016-10-31
    
    userActionLog
        .join(userBaseInfo, userActionLog("userId") === userBaseInfo("userId")) 
        .filter(userBaseInfo("registTime") >= "2016-10-01"
            && userBaseInfo("registTime") <= "2016-10-31"
            && userActionLog("actionTime") >= userBaseInfo("registTime")
            && userActionLog("actionTime") <= date_add(userBaseInfo("registTime"), 7)
            && userActionLog("actionType") === 0)
        .groupBy(userBaseInfo("userId"), userBaseInfo("username"))
        .agg(count(userActionLog("logId")).alias("actionCount"))
        .sort($"actionCount".desc)
        .limit(10)
        .show()
    
    userActionLog
        .join(userBaseInfo, userActionLog("userId") === userBaseInfo("userId")) 
        .filter(userBaseInfo("registTime") >= "2016-10-01"
            && userBaseInfo("registTime") <= "2016-10-31"
            && userActionLog("actionTime") >= userBaseInfo("registTime")
            && userActionLog("actionTime") <= date_add(userBaseInfo("registTime"), 7)
            && userActionLog("actionType") === 1)
        .groupBy(userBaseInfo("userId"), userBaseInfo("username"))
        .agg(round(sum(userActionLog("purchaseMoney")),2).alias("purchaseMoneyTotal")) 
        .sort($"purchaseMoneyTotal".desc)
        .limit(10)
        .show() 
  }
  
}


```

